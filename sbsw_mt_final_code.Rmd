---
title: "sbsw_mt_final_code"
output: html_document
date: "2025-01-14"
---

This document contains code used for analysis and figure generation for the paper "Light cues drive community-wide transcriptional shifts in a hypersaline saltern". If you have any questions, please contact the corresponding author at mw1144@georgetown.edu (Margaret Weng). 

##Read mapping and preprocessing##

#Mapping reads to MAGs and obtaining transcript tallies

```{bash, eval=F}

#!/bin/bash
while read -r line; do

bwa mem -t 45 /home/m1weng/sbsw/mt_analysis_v2/export/210924_bins.fasta ${line}_R1_001.qc.fastq.gz ${line}_R2_001.qc.fastq.gz > ${line}.sam #use bwa to map all reads to a concatenated bin database

gzip ${line}.sam

python3 parse_sam.py ${line}

done<mt_filenames.txt
```

#Count number of qc passed reads and percent mapped
Use the 'samtools flagstat' tool of the samfiles generated by BWA
```{bash, eval=F}

#!/bin/bash
#SBATCH --job-name=sbsw_readstats
#SBATCH --output=z01.%x
#SBATCH --mail-type=END,FAIL --mail-user=mw1144@georgetown.edu
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8 # Max is 64 cores (32 hyper threaded processors)
#SBATCH --time=24:00:00 #48 hrs is the max
#SBATCH --mem=32G # a value of 0 here will allocate all available memory
#SBATCH --partition singleGPU

#load environment
module load miniconda3
conda init bash
source ~/.bashrc
conda activate sbsw

while read -r line; do

samtools flagstat ${line}.sam.gz > ${line}_stats.txt

done<sbsw_filenames.txt

```


#Importing the results from bwa mapping into R for downstream analysis with DEseq2

aggregate individual csvs into one large data file and reformat:
```{r}
library(tidyverse)

samplenames <-scan(file="mt_data_v2/mt_tally_v2/mt_filenames.txt", what=character())

data <-list.files(path="mt_data_v2/mt_tally_v2", pattern="*_tally.csv", full.names=TRUE) %>% 
  lapply(read_csv) %>% mapply(cbind, ., "SampleID"=samplenames, SIMPLIFY=F)


mt_tally <- bind_rows(data, .id= NULL) %>% pivot_wider(.,
                       names_from=SampleID,
                       values_from=n_hits) %>%rename("position"="...1")


#Get rid of NAs in the counts only, not genes!

mt_tally[,c(5:40)][is.na(mt_tally[,c(5:40)])] <- 0

saveRDS(mt_tally, file="mt_tally_v2.rds")
write_csv(mt_tally, "mt_tally_v2.csv")


```

#Circadian normalization of time points
our time is linear in this file (assumes 6am is the farthest point from 10am, which is untrue)--we must circularize using trigonomic predictors
The cosinor model: Yt=ccos(wt)+ssin(wt) where t=1,...n

```{r}

#full trig version we heart this 
#V2.0 time normalization

#Import a metadata file that has the sample conditions (created in Excel)

metadata <- read.csv("sample_conditions.csv", header=TRUE)


#Append a column to the metadata file with normalized time infomration

timecount_norm <-c()

for(i in metadata$timecount_noon_centered){
  time.norm <- (2 * pi * i)/24
  time.norm.trig <-cos(time.norm) + sin(time.norm)
  timecount_norm <- append(timecount_norm, time.norm.trig)
}

metadata_norm_v2 <- cbind(metadata, timecount_norm)

ggplot(metadata_norm_v2, aes(x=time24, y=timecount_norm)) + geom_point()

write.csv(metadata_norm_v2, "metadata_norm_v2.csv")


metadata_norm_v2 <-read_csv("metadata_norm_v2.csv")

```

#correcting for a batch effect from two different rounds of RNA extractions

#install and load packages
```{r}
BiocManager::install("limma")
library(limma)
library(tidyverse)
```

#Load data
```{r}

mt_tally <-read_csv("../mt_data_v2/mt_tally_v2.csv") #non-normalized transcript counts
metadata_norm_v2 <-read_csv("metadata_norm_v2.csv")
normalized_counts <- read_csv("../mt_data_v2/normalized_mt_tally_v2_genes.csv")

#create matrix from mt_tally and call this mt_raw
mt_raw <- mt_tally[,c(1, 5:40)]
mt_raw <- as.data.frame(mt_raw)
rownames(mt_raw) <-mt_raw$position
mt_raw[,"position"] <- NULL

```

Batch correction steps:

1. Determine the scope of the batch effect using full vs reduced model in deseq2
2. Use LIMMA to remove batch effect from variance stabilized counts 
3. Rerun PCA plot and test statistical significance of remaining batch effects, if any
4. If LIMMA is successfully able to remove batch effects from variance stabilized data, we can assume that deseq2 is able to model these effects internally when batch is included in the reduced model. 

#Visualizing the batch effect on variance-stabilized data
PCA test to visualize batch effects on the data in DESeq:
```{r}

dds <- DESeqDataSetFromMatrix(countData=mt_raw,
                              colData=metadata_norm_v2,
                              design=~extraction_batch + timecount_norm) #load deseq2 dataset 


dds <- estimateSizeFactors( dds ) #normalizing data by gene length and rna counts

sizeFactors(dds)

vsd <-vst(dds, blind=FALSE) #variance stabilizing transformation that is not blind to design. Design is used to calculate variability but not remove it 

plotPCA(vsd, intgroup=c("time24", "extraction_batch")) + scale_color_brewer(palette="Paired") #visualize the PCA plot of data before batch effects are removed-- batch accounts for ~22% of the variation and is a significant variable

```

Using limma to remove batch effects from the variance stabilized count matrix:

```{r}
mat <- assay(vsd)
mm <- model.matrix(~timecount_norm, colData(vsd))
mat <- limma::removeBatchEffect(mat, batch=vsd$extraction_batch, design=mm)
assay(vsd) <- mat
plotPCA(vsd, intgroup=c("time24", "extraction_batch")) + scale_color_brewer(palette="Paired")

#this limma-corrected matrix can be used for downstream applications independent of deseq2 such as pca, statistical testing, etc. it should NOT be used as the input for deseq2 as it is not raw count data! deseq2 will remove batch effects internally if they are included in the design 


write.csv(mat, "limma_corrected_counts.csv")
```

This limma-corrected matrix can be used for downstream applications independent of deseq2 such as pca, statistical testing, etc. it should NOT be used as the input for deseq2 as it is not raw count data! deseq2 will account for batch effects internally if they are included in the design. 

Anova to test whether batch is still significant:
```{r}
#create distance matrix
sampleDists <- dist(t(assay(vsd)))
sampleDistMatrix <- as.matrix(sampleDists)

#using adonis
adonis(sampleDistMatrix ~ timecount_norm + extraction_batch, data=metadata_norm_v2, permutations=999)
```

The batch effects are no longer statistically significant!

Limma has shown that we can correct for batch effects. Now DESeq2 will remove them internally if they are included in the design. We do NOT use the limma data for DESeq2, just the raw count matrix. Limma is just to prove that batch correction can be done successfully 

##Running DESeq2 differential expression analysis while correcting for batch effect in the LRT model##

NOTE: tests of log fold change above or below a theshhold must be Wald tests. Therefore, I am unable to use LFCthreshold as recommended to filter for fold change above or below a certain threshold. I will need to do this on the results table. 

```{r}

#load data
mt_tally <-read_csv("mt_data_v2/mt_tally_v2.csv")
metadata_norm_v2 <-read_csv("metadata_norm_v2.csv")
normalized_counts <- read_csv("mt_data_v2/normalized_mt_tally_v2_genes.csv")

#get a list of all bins
bins <-unique(mt_tally$bin)

#set position as the rownames and keep bins as a column to loop through bin by bin
mt_tally_bins <-mt_tally[,-c(3:4)]
mt_tally_bins <- as.data.frame(mt_tally_bins)
row.names(mt_tally_bins) <-mt_tally_bins$position
mt_tally_bins[,"position"]<-NULL

#list to hold all differentially expressed genes by bin
comparisons_master <- list() 
genomes_mt <-c()
g<-0


#loop through all bins
for(i in bins) {
  g <- g+1
  mt_bin <- mt_tally_bins %>% filter(bin == i)

mt_matrix <- mt_bin[,-1]
mt_matrix <- as.data.frame(mt_matrix)

dds <- DESeqDataSetFromMatrix(countData=mt_matrix,
                              colData=metadata_norm_v2,
                              design=~extraction_batch + timecount_norm)
dds_lrt_time <- DESeq(dds, test="LRT", reduced = ~extraction_batch)
res_lrt <- results(dds_lrt_time)

res_data <-matrix(nrow=length(res_lrt@listData[["log2FoldChange"]]), ncol=0, data=0)
res_data <-data.frame(res_data)

res_data$log2FoldChange <- res_lrt@listData[["log2FoldChange"]]
res_data$baseMean <- res_lrt@listData[["baseMean"]]
res_data$padj <-res_lrt@listData[["padj"]]
res_data$gene <-res_lrt@rownames
res_data$genome <- i


comparisons_master <- c(list(res_data), comparisons_master) ## or append(x, list(df), 0)

#rename list item with name of genome

genomes_mt <-append(genomes_mt, i)
}

comparisons_master_df <-plyr::ldply(comparisons_master, rbind) %>% rename(., "gene"="position")
master <-comparisons_master_df %>% left_join(.,
                                             normalized_counts[,c(37:40)],
                                             by="position") %>%select(-"genome")


#with LFC threshold 
summary(res_lrt)
res_filtered <-results(dds_lrt_time, lfcThreshold=1)

write_csv(master, "master_diffex_genes_blocked.csv")
gtdbtk_tax <-read_csv("mt_data_v2/gtdbtk_results/gtdbtk_full.csv")

master_tax <-left_join(master, gtdbtk_tax[,c(1:8)], by="bin")
write.csv(master_tax, "master_genes_taxonomy_gtdbtk_blocked.csv")

diffex_tax <-master_tax %>% filter(padj < 0.05)
write.csv(diffex_tax, "diffex_tax_blocked.csv")
```



